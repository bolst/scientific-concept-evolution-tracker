{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fa55c5",
   "metadata": {},
   "source": [
    "# Scientific Concept Evolution Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2833a6",
   "metadata": {},
   "source": [
    "At this point I feel like I've experimented enough with this and now I can implement something that makes sense to me. This notebook will serve as a source of documentation for the implementation of this project. I will document my thoughts of how I will approach building this project (which I will refer to as SCET going forward).\n",
    "\n",
    "The goal of this project is as follows. Given a scientific concept (i.e., the query) such as \"Corona\", how can we\n",
    "\n",
    "1. Find the most relevant papers from different eras that indicate \"shifts\" in the contextual meaning of the term.\n",
    "    - Before 2020, \"Corona\" would likely be associated with [corona discharge](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://en.wikipedia.org/wiki/Corona_discharge&ved=2ahUKEwid06-J3rCRAxWkOzQIHcWSLkEQFnoECBoQAQ&usg=AOvVaw0VzdLEHNdHES3fu02lie58) or [stellar corona](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://en.wikipedia.org/wiki/Stellar_corona&ved=2ahUKEwid06-J3rCRAxWkOzQIHcWSLkEQFnoECBsQAQ&usg=AOvVaw0VxSEegT2Km54Q3L-7KW9Z), where nowadays it is more associated with virology\n",
    "2. Design a mechanism that can re-rank those papers based on how \"pivotal\" it was for the history of the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e78d9f",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "As of 2022 there are over 5.14 million academic articles published per year ([WordsRated](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://wordsrated.com/number-of-academic-papers-published-per-year/&ved=2ahUKEwiQqbOC3bCRAxWMCjQIHWSxBiMQFnoECCEQAQ&usg=AOvVaw0fqfpJLzxavGNIlmsVlrxX)), and the cumulative total is estimated to be over 50 million ([ResearchGate](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.researchgate.net/publication/229062236_Article_50_million_An_estimate_of_the_number_of_scholarly_articles_in_existence&ved=2ahUKEwiQqbOC3bCRAxWMCjQIHWSxBiMQFnoECB8QAQ&usg=AOvVaw112p_TvuMPfBcb1QdINC74)).\n",
    "\n",
    "There are various sources where we can obtain publication data. I arbitrarily chose to source my data from [arXiv](https://arxiv.org/), mainly because we can do a bulk download of the metadata of ~2.8m publications. We can do this via [Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv) or directly through their Google Cloud Storage bucket at `gs://arxiv-dataset/metadata-v5/arxiv-metadata-oai.json`, either option is ~4GB. Using this, we can get a publication's\n",
    "- `id` (the arXiv identifier for the publication)\n",
    "- `doi`, although this seems to be inconsistently populated\n",
    "- `category` (all categories listed [here](https://arxiv.org/category_taxonomy))\n",
    "- `title`\n",
    "- `abstract`\n",
    "- `authors`\n",
    "\n",
    "There are two features that this dataset does not contain: full-text and citations. Of course, these properties seem to be the most difficult to obtain for a publication. Full-text is justifiably hard to obtain because it is relatively difficult to extract text from PDFs. Citations are another problem in itself. I am going to try to get by without using full-text and citations, although I acknowledge that I may have to factor these into the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e433d",
   "metadata": {},
   "source": [
    "> *Note:* I am going to use [Attention Is All You Need](https://arxiv.org/abs/1706.03762) as an example throughout this notebook. It serves as an easy benchmark. If I query `Transformer` or `Transformer Attention`, then this paper definitely should be the most pivotal..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fa962",
   "metadata": {},
   "source": [
    "I have already ingested the arXiv metadata into a Postgres database (see [ingest_metadata.py](../scripts/ingest_metadata.py)). I don't want to experiment building SCET with all of this data (my hardware does not have unlimited memory and I do not have all the time in the world), so I will use a subset of this data in hopes that it will scale.\n",
    "\n",
    "I will choose this subset by sampling over time and categories. Including the available time range will allow us to test evolution of a concept, and the variety of categories should put enough noise into the corpus to make it non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a36dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "DB_USER = os.environ[\"POSTGRES_USER\"]\n",
    "DB_PASSWORD = os.environ[\"POSTGRES_PASSWORD\"]\n",
    "DB_HOST = os.environ[\"POSTGRES_HOST\"]\n",
    "DB_PORT = os.environ[\"POSTGRES_PORT\"]\n",
    "DB_NAME = os.environ[\"POSTGRES_DB\"]\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "with engine.connect():\n",
    "    print(f\"Connected to PostgreSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4060dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['acc-phys', 'adap-org', 'alg-geom', 'ao-sci', 'astro-ph']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = '''\n",
    "SELECT DISTINCT\n",
    "\tprimary_category\n",
    "FROM\n",
    "\tpapers\n",
    "ORDER BY\n",
    "\tprimary_category\n",
    "'''\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(text(sql)).fetchall()\n",
    "    categories = [result.primary_category for result in results]\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "categories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd5993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates range from 1986-04-25 to 2025-11-26\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "SELECT\n",
    "\tmin(published_date) AS min_date,\n",
    "\tmax(published_date) AS max_date\n",
    "FROM\n",
    "\tpapers\n",
    "'''\n",
    "with engine.connect() as connection:\n",
    "    min_date, max_date = connection.execute(text(sql)).fetchone()\n",
    "print(f\"Dates range from {min_date} to {max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad65df6",
   "metadata": {},
   "source": [
    "### Generate Sample Data\n",
    "\n",
    "We will use a materialized view to store our sample data (this way it only needs to be generated once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8641b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized view created\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS sample_papers AS\n",
    "WITH\n",
    "\tparams AS (\n",
    "\t\tSELECT\n",
    "\t\t\t5000 AS target_size,\n",
    "\t\t\t50 AS num_time_buckets,\n",
    "\t\t\tmin(published_date) AS min_date,\n",
    "\t\t\tmax(published_date) AS max_date\n",
    "\t\tFROM papers\n",
    "\t),\n",
    "\tglobal_stats AS (\n",
    "\t\t-- Calculate global distribution of primary_category\n",
    "\t\tSELECT\n",
    "\t\t\tprimary_category,\n",
    "\t\t\tCOUNT(*)::NUMERIC / SUM(COUNT(*)) OVER () AS global_ratio\n",
    "\t\tFROM papers\n",
    "\t\tWHERE\n",
    "\t\t\tpublished_date BETWEEN ( SELECT min_date FROM params ) AND ( SELECT max_date FROM params )\n",
    "\t\tGROUP BY primary_category\n",
    "\t),\n",
    "\tbinned_data AS (\n",
    "\t\t-- assign every row to a time bucket with global distributions\n",
    "\t\tSELECT\n",
    "\t\t\tt.*,\n",
    "\t\t\tgs.global_ratio,\n",
    "\t\t\t-- equal-width time buckets\n",
    "\t\t\twidth_bucket(\n",
    "\t\t\t\tEXTRACT( epoch FROM t.published_date ),\n",
    "\t\t\t\tEXTRACT( epoch FROM ( SELECT min_date FROM params) ),\n",
    "\t\t\t\tEXTRACT( epoch FROM (SELECT max_date FROM params) ),\n",
    "\t\t\t\t(SELECT num_time_buckets FROM params)\n",
    "\t\t\t) AS bucket_id\n",
    "\t\tFROM\n",
    "\t\t\tpapers t\n",
    "\t\t\tJOIN global_stats gs ON t.primary_category = gs.primary_category\n",
    "\t\tWHERE\n",
    "\t\t\tt.published_date BETWEEN ( SELECT min_date FROM params ) AND ( SELECT max_date FROM params )\n",
    "\t),\n",
    "\ttargets AS (\n",
    "\t\t-- calculate exactly how many rows we need for each (Bucket, Category) pair\n",
    "\t\t-- determine via (Total Sample / Total Buckets) * Category Frequency\n",
    "\t\tSELECT\n",
    "\t\t\t*,\n",
    "\t\t\t(\n",
    "\t\t\t\t(\n",
    "\t\t\t\t\tSELECT target_size\n",
    "\t\t\t\t\tFROM params\n",
    "\t\t\t\t) / (\n",
    "\t\t\t\t\tSELECT num_time_buckets\n",
    "\t\t\t\t\tFROM params\n",
    "\t\t\t\t)::NUMERIC\n",
    "\t\t\t) * global_ratio AS exact_target\n",
    "\t\tFROM binned_data\n",
    "\t),\n",
    "\tranked_data AS (\n",
    "\t\t-- shuffle rows randomly within their specific (Bucket, Category) group\n",
    "\t\tSELECT\n",
    "\t\t\t*,\n",
    "\t\t\tROW_NUMBER() OVER (\n",
    "\t\t\t\tPARTITION BY\n",
    "\t\t\t\t\tbucket_id,\n",
    "\t\t\t\t\tprimary_category\n",
    "\t\t\t\tORDER BY\n",
    "\t\t\t\t\t-- ensure that Attention Is All You Need is included\n",
    "\t\t\t\t\tCASE WHEN arxiv_id = '1706.03762' THEN 0 ELSE 1 END ASC,\n",
    "\t\t\t\t\tRANDOM()\n",
    "\t\t\t) AS rnk\n",
    "\t\tFROM targets\n",
    "\t)\n",
    "SELECT * FROM ranked_data\n",
    "WHERE\n",
    "\trnk <= floor(exact_target) + (\n",
    "\t\tCASE WHEN RANDOM() < (exact_target - floor(exact_target)) THEN 1 ELSE 0 END\n",
    "\t);\n",
    "'''\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(sql))\n",
    "    connection.commit()\n",
    "print(\"Materialized view created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655aa8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3606, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sql = '''\n",
    "SELECT\n",
    "\tarxiv_id,\n",
    "\ttitle,\n",
    "\tabstract,\n",
    "\tprimary_category,\n",
    "\tpublished_date,\n",
    "\tdoi\n",
    "FROM\n",
    "\tsample_papers\n",
    "ORDER BY\n",
    "\tpublished_date\n",
    "'''\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(text(sql))\n",
    "    papers = pd.DataFrame(results.fetchall())\n",
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a241e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-th/9108028</td>\n",
       "      <td>Applied Conformal Field Theory</td>\n",
       "      <td>These lectures consisted of an elementary intr...</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>1988-11-11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math/9201207</td>\n",
       "      <td>The Rademacher cotype of operators from $l_\\in...</td>\n",
       "      <td>We show that for any operator $T:l_\\infty^N\\to...</td>\n",
       "      <td>math.FA</td>\n",
       "      <td>1989-11-17</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs/9301111</td>\n",
       "      <td>Nested satisfiability</td>\n",
       "      <td>A special case of the satisfiability problem, ...</td>\n",
       "      <td>cs.CC</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>math/9201303</td>\n",
       "      <td>Stable husbands</td>\n",
       "      <td>Suppose $n$ boys and $n$ girls rank each other...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>math/9201271</td>\n",
       "      <td>Conformal dynamics problem list</td>\n",
       "      <td>This is a list of unsolved problems given at t...</td>\n",
       "      <td>math.DS</td>\n",
       "      <td>1990-01-18</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         arxiv_id                                              title  \\\n",
       "0  hep-th/9108028                     Applied Conformal Field Theory   \n",
       "1    math/9201207  The Rademacher cotype of operators from $l_\\in...   \n",
       "2      cs/9301111                              Nested satisfiability   \n",
       "3    math/9201303                                    Stable husbands   \n",
       "4    math/9201271                    Conformal dynamics problem list   \n",
       "\n",
       "                                            abstract primary_category  \\\n",
       "0  These lectures consisted of an elementary intr...           hep-th   \n",
       "1  We show that for any operator $T:l_\\infty^N\\to...          math.FA   \n",
       "2  A special case of the satisfiability problem, ...            cs.CC   \n",
       "3  Suppose $n$ boys and $n$ girls rank each other...          math.CO   \n",
       "4  This is a list of unsolved problems given at t...          math.DS   \n",
       "\n",
       "  published_date   doi  \n",
       "0     1988-11-11  None  \n",
       "1     1989-11-17  None  \n",
       "2     1990-01-01  None  \n",
       "3     1990-01-01  None  \n",
       "4     1990-01-18  None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110c97e",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "Now that we have our sample data, we will convert the raw text into vector representations that we can search with. We will use a hybrid embedding strategy to capture both semantic meaning and specific keyword matches.\n",
    "\n",
    "1.  *Dense Embeddings*: [SPECTER2](https://huggingface.co/allenai/specter2_base) is a model specifically trained on scientific citations. We will use it for capturing the high-level semantic meaning of a paper. If we query \"Corona\", SPECTER2 will help us find papers about viral structures or stellar atmospheres (depending on the context), rather than just keyword matches\n",
    "2.  *Sparse Embeddings*: [SPLADE](https://github.com/naver/splade) generates sparse vectors that function like a \"learned bag-of-words\". It performs expansion (adding relevant terms not present in the text) and re-weighting. This is useful for maintaining keyword precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46720ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/specter2_base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dense Model: allenai/specter2_base\n",
      "Loaded Sparse Model: naver/splade-cocondenser-ensembledistil\n",
      "Loaded Sparse Model: naver/splade-cocondenser-ensembledistil\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# load dense model (SPECTER2)\n",
    "dense_model_name = 'allenai/specter2_base'\n",
    "dense_model = SentenceTransformer(dense_model_name).to(device)\n",
    "print(f\"Loaded Dense Model: {dense_model_name}\")\n",
    "\n",
    "# load sparse model (SPLADE)\n",
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "sparse_tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)\n",
    "sparse_model = AutoModelForMaskedLM.from_pretrained(sparse_model_id).to(device)\n",
    "print(f\"Loaded Sparse Model: {sparse_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0ffb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from CSV...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>dense_vector</th>\n",
       "      <th>sparse_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-th/9108028</td>\n",
       "      <td>[0.022522805, 0.029086195, 0.005951591, 0.0440...</td>\n",
       "      <td>{1012: 0.004072231240570545, 1016: 0.634854495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math/9201207</td>\n",
       "      <td>[0.03122084, 0.03126316, -0.00011172818, 0.025...</td>\n",
       "      <td>{1002: 1.6506469249725342, 1003: 0.30638721585...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs/9301111</td>\n",
       "      <td>[0.043976273, 0.0058500934, 0.014620507, -0.00...</td>\n",
       "      <td>{1000: 0.3820948600769043, 1005: 0.00831570103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>math/9201303</td>\n",
       "      <td>[0.022883516, 0.02199802, -0.008349336, 0.0087...</td>\n",
       "      <td>{1002: 1.385781168937683, 1003: 0.347594857215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>math/9201271</td>\n",
       "      <td>[0.011540209, 0.0029417637, 0.011269698, 0.036...</td>\n",
       "      <td>{2001: 0.396415650844574, 2012: 0.000928447640...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         arxiv_id                                       dense_vector  \\\n",
       "0  hep-th/9108028  [0.022522805, 0.029086195, 0.005951591, 0.0440...   \n",
       "1    math/9201207  [0.03122084, 0.03126316, -0.00011172818, 0.025...   \n",
       "2      cs/9301111  [0.043976273, 0.0058500934, 0.014620507, -0.00...   \n",
       "3    math/9201303  [0.022883516, 0.02199802, -0.008349336, 0.0087...   \n",
       "4    math/9201271  [0.011540209, 0.0029417637, 0.011269698, 0.036...   \n",
       "\n",
       "                                       sparse_vector  \n",
       "0  {1012: 0.004072231240570545, 1016: 0.634854495...  \n",
       "1  {1002: 1.6506469249725342, 1003: 0.30638721585...  \n",
       "2  {1000: 0.3820948600769043, 1005: 0.00831570103...  \n",
       "3  {1002: 1.385781168937683, 1003: 0.347594857215...  \n",
       "4  {2001: 0.396415650844574, 2012: 0.000928447640...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def generate_dense_embeddings(texts):\n",
    "    return dense_model.encode(texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "def generate_sparse_embedding(text):\n",
    "    tokens = sparse_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = sparse_model(**tokens)\n",
    "    \n",
    "    # SPLADE aggregation (max pooling over log(1 + relu(logits)))\n",
    "    logits = output.logits\n",
    "    values, _ = torch.max(\n",
    "        torch.log(1 + torch.relu(logits)) * tokens.attention_mask.unsqueeze(-1), \n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Convert to dict ~ {token_id: weight}\n",
    "    vector = values.squeeze().cpu().numpy()\n",
    "    non_zero_indices = vector.nonzero()[0]\n",
    "    return {int(idx): float(vector[idx]) for idx in non_zero_indices}\n",
    "\n",
    "def generate_embeddings(paper_df: pd.DataFrame, load_csv: bool = True, save_csv: bool = True):\n",
    "    csv_path = 'data/sample_papers_with_embeddings.csv'\n",
    "    \n",
    "    if load_csv:\n",
    "        print(\"Loading from CSV...\")\n",
    "        try:\n",
    "            def parse_dense_vector_csv(s):\n",
    "                s = s.replace('[', '').replace(']', '').replace('\\n', ' ')\n",
    "                return np.fromstring(s, sep=' ', dtype=np.float32)\n",
    "            csv_df = pd.read_csv(csv_path)\n",
    "            csv_df.dense_vector = csv_df.dense_vector.apply(parse_dense_vector_csv)\n",
    "            csv_df.sparse_vector = csv_df.sparse_vector.apply(literal_eval)\n",
    "            csv_df.published_date = csv_df.published_date.apply(pd.to_datetime)\n",
    "            return csv_df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load from CSV: {e}\")\n",
    "    \n",
    "    df = paper_df.copy()\n",
    "\n",
    "    print(\"Generating Dense Embeddings...\")\n",
    "    sep_token = dense_model.tokenizer.sep_token\n",
    "    df['specter_input'] = df['title'] + sep_token + df['abstract']\n",
    "    df['dense_vector'] = list(\n",
    "        generate_dense_embeddings(df['specter_input'].tolist()).cpu().numpy()\n",
    "    )\n",
    "\n",
    "    print(\"Generating Sparse Embeddings...\")\n",
    "    tqdm.pandas()\n",
    "    df['sparse_vector'] = df['specter_input'].progress_apply(generate_sparse_embedding)\n",
    "\n",
    "    print(\"Embeddings generated.\")\n",
    "    \n",
    "    if save_csv:\n",
    "        papers.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved embeddings to {csv_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "embedded_papers = generate_embeddings(papers)\n",
    "embedded_papers[['arxiv_id', 'dense_vector', 'sparse_vector']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc516f7d",
   "metadata": {},
   "source": [
    "## Vector Database Setup (Milvus)\n",
    "\n",
    "We will use [Milvus](https://milvus.io/) as the vector database to store our embeddings.\n",
    "\n",
    "We need to define a schema for our collection. A key design choice here is a partition key. We want to be able to efficiently query papers from specific time periods/categories. By creating a partition key derived from the `Year` and `Category` columns, Milvus can physically group related data, speeding up our temporal queries.\n",
    "\n",
    "We use `Int64` for the `paper_id` (Primary Key) instead of the raw string `arxiv_id` for performance (lookup). We do not use `VarChar` since we have no guarantee on the length of the arXiv ID. We will also store the original `arxiv_id` as a string field for easier debugging and retrieval, even though we are not using it as the primary key.\n",
    "\n",
    "**Schema:**\n",
    "*   `paper_id` (Int64): A 64-bit hashed integer version of the arXiv ID (Primary Key)\n",
    "*   `arxiv_id` (VarChar): The original arXiv ID string (for debugging/reference)\n",
    "*   `dense_vector` (FloatVector, 768): The SPECTER2 embedding\n",
    "*   `sparse_vector` (SparseFloatVector): The SPLADE embedding\n",
    "*   `publication_year` (Int16): For filtering\n",
    "*   `partition_key` (Int64): Hash of `Year` + `Category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66b21a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Milvus at 10.182.207.119:19530\n",
      "Dropping existing collection: arxiv_diachronic_sample\n",
      "Creating indexes...\n",
      "Creating indexes...\n",
      "Collection 'arxiv_diachronic_sample' created and indexed.\n",
      "Collection 'arxiv_diachronic_sample' created and indexed.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "# Connect to Milvus\n",
    "MILVUS_HOST = os.environ.get(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.environ.get(\"MILVUS_PORT\", \"19530\")\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "print(f\"Connected to Milvus at {MILVUS_HOST}:{MILVUS_PORT}\")\n",
    "\n",
    "collection_name = \"arxiv_diachronic_sample\"\n",
    "\n",
    "# Drop if exists to start fresh\n",
    "if utility.has_collection(collection_name):\n",
    "    print(f\"Dropping existing collection: {collection_name}\")\n",
    "    utility.drop_collection(collection_name)\n",
    "\n",
    "# Define Schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"paper_id\", dtype=DataType.INT64, is_primary=True, description=\"Hashed arXiv ID\"),\n",
    "    FieldSchema(name=\"arxiv_id\", dtype=DataType.VARCHAR, max_length=32, description=\"Original arXiv ID\"),\n",
    "    FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=768, description=\"SPECTER2 embedding\"),\n",
    "    FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR, description=\"SPLADE embedding\"),\n",
    "    FieldSchema(name=\"publication_year\", dtype=DataType.INT16, description=\"Year for filtering\"),\n",
    "    FieldSchema(name=\"partition_key\", dtype=DataType.INT64, is_partition_key=True, description=\"Hash of Year + Domain\"),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, \"Diachronic ArXiv Collection\")\n",
    "collection = Collection(collection_name, schema)\n",
    "\n",
    "# Create Indexes\n",
    "print(\"Creating indexes...\")\n",
    "collection.create_index(field_name=\"dense_vector\", index_params={\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"index_type\": \"HNSW\",\n",
    "    \"params\": {\"M\": 16, \"efConstruction\": 200}\n",
    "})\n",
    "collection.create_index(field_name=\"sparse_vector\", index_params={\n",
    "    \"metric_type\": \"IP\", # Inner Product is standard for sparse\n",
    "    \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "    \"params\": {\"drop_ratio_build\": 0.2}\n",
    "})\n",
    "collection.create_index(field_name=\"publication_year\", index_name=\"year_index\")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created and indexed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac008380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 3606 records for ingestion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3606/3606 [00:00<00:00, 28081.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into Milvus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete. Collection row count: 3606\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_to_int64(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Generates a stable 64-bit integer hash from a string.\n",
    "    We use MD5 and take the first 15 hex characters (60 bits) to ensure it fits \n",
    "    within a signed 64-bit integer (Milvus Int64 limit).\n",
    "    \"\"\"\n",
    "    return int(hashlib.md5(s.encode()).hexdigest()[:15], 16)\n",
    "\n",
    "def get_partition_key(year, domain_str) -> int:\n",
    "    key_str = f\"{year}_{domain_str}\"\n",
    "    return hash_to_int64(key_str)\n",
    "\n",
    "def prepare_and_ingest(df, collection):\n",
    "    insert_data = {\n",
    "        \"paper_id\": [],\n",
    "        \"arxiv_id\": [],\n",
    "        \"dense_vector\": [],\n",
    "        \"sparse_vector\": [],\n",
    "        \"publication_year\": [],\n",
    "        \"partition_key\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"Preparing {len(df)} records for ingestion...\")\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            # Generate IDs using 64-bit hash\n",
    "            p_id = hash_to_int64(row['arxiv_id'])\n",
    "            year = row['published_date'].year\n",
    "            part_key = get_partition_key(year, row['primary_category'])\n",
    "            \n",
    "            insert_data[\"paper_id\"].append(p_id)\n",
    "            insert_data[\"arxiv_id\"].append(str(row['arxiv_id']))\n",
    "            insert_data[\"dense_vector\"].append(row['dense_vector'])\n",
    "            insert_data[\"sparse_vector\"].append(row['sparse_vector'])\n",
    "            insert_data[\"publication_year\"].append(year)\n",
    "            insert_data[\"partition_key\"].append(part_key)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['arxiv_id']}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(\"Inserting into Milvus...\")\n",
    "    collection.upsert([\n",
    "        insert_data[\"paper_id\"],\n",
    "        insert_data[\"arxiv_id\"],\n",
    "        insert_data[\"dense_vector\"],\n",
    "        insert_data[\"sparse_vector\"],\n",
    "        insert_data[\"publication_year\"],\n",
    "        insert_data[\"partition_key\"]\n",
    "    ])\n",
    "    \n",
    "    collection.flush()\n",
    "    print(f\"Ingestion complete. Collection row count: {collection.num_entities}\")\n",
    "\n",
    "prepare_and_ingest(embedded_papers, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e693ef3",
   "metadata": {},
   "source": [
    "## Hybrid Search Implementation\n",
    "\n",
    "Now we can implement the search logic. We will use a hybrid search:\n",
    "1.  *Dense vector search*: Finds semantically similar papers (good for concept matching)\n",
    "2.  *Sparse vector search*: Finds keyword-based matches (good for precision)\n",
    "\n",
    "We use a `WeightedRanker` to combine these scores. We can adjust the `alpha` parameter to weight dense vs. sparse importance.\n",
    "\n",
    "Note: in our hybrid search, we perform two independent searches (Dense and Sparse) and then combine the results. If we set the limit too low (e.g., 10), a paper that is ranked 15th in Dense and 15th in Sparse might not be retrieved by *either* search, even though its combined score might be higher than the top results. To fix this, we fetch a larger pool of candidates (e.g., 200) from each method before reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5130abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying: 'Transformer Attention'\n",
      "Success! 'Attention Is All You Need' found at rank 7 with score 0.9266\n",
      "Success! 'Attention Is All You Need' found at rank 7 with score 0.9266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2109.07799</td>\n",
       "      <td>Label-Attention Transformer with Geometrically...</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>0.945767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1710.10504</td>\n",
       "      <td>Phase Conductor on Multi-layered Attentions fo...</td>\n",
       "      <td>2017-10-28</td>\n",
       "      <td>0.937934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2410.15198</td>\n",
       "      <td>R-GAT: Cancer Document Classification Leveragi...</td>\n",
       "      <td>2024-10-19</td>\n",
       "      <td>0.935711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2412.18614</td>\n",
       "      <td>Investigating Acoustic-Textual Emotional Incon...</td>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>0.931328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2201.06876</td>\n",
       "      <td>Syntax-based data augmentation for Hungarian-E...</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>0.929613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2202.03548</td>\n",
       "      <td>HeadPosr: End-to-end Trainable Head Pose Estim...</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>0.927467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1706.03762</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>0.926582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1907.04294</td>\n",
       "      <td>An Attention Mechanism for Musical Instrument ...</td>\n",
       "      <td>2019-07-09</td>\n",
       "      <td>0.926103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1908.04626</td>\n",
       "      <td>Attention is not not Explanation</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>0.925592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2411.07463</td>\n",
       "      <td>MSEG-VCUQ: Multimodal SEGmentation with Enhanc...</td>\n",
       "      <td>2024-11-12</td>\n",
       "      <td>0.924748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv_id                                              title  \\\n",
       "0  2109.07799  Label-Attention Transformer with Geometrically...   \n",
       "1  1710.10504  Phase Conductor on Multi-layered Attentions fo...   \n",
       "2  2410.15198  R-GAT: Cancer Document Classification Leveragi...   \n",
       "3  2412.18614  Investigating Acoustic-Textual Emotional Incon...   \n",
       "4  2201.06876  Syntax-based data augmentation for Hungarian-E...   \n",
       "5  2202.03548  HeadPosr: End-to-end Trainable Head Pose Estim...   \n",
       "6  1706.03762                          Attention Is All You Need   \n",
       "7  1907.04294  An Attention Mechanism for Musical Instrument ...   \n",
       "8  1908.04626                   Attention is not not Explanation   \n",
       "9  2411.07463  MSEG-VCUQ: Multimodal SEGmentation with Enhanc...   \n",
       "\n",
       "  published_date     score  \n",
       "0     2021-09-16  0.945767  \n",
       "1     2017-10-28  0.937934  \n",
       "2     2024-10-19  0.935711  \n",
       "3     2024-12-09  0.931328  \n",
       "4     2022-01-18  0.929613  \n",
       "5     2022-02-07  0.927467  \n",
       "6     2017-06-12  0.926582  \n",
       "7     2019-07-09  0.926103  \n",
       "8     2019-08-13  0.925592  \n",
       "9     2024-11-12  0.924748  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import AnnSearchRequest, WeightedRanker\n",
    "\n",
    "def search_papers(\n",
    "    query: str, \n",
    "    start_year: int = 1980, \n",
    "    end_year: int = 2025, \n",
    "    limit: int = 10, \n",
    "    alpha: float = 0.5\n",
    "    ):\n",
    "    # embed query\n",
    "    dense_vec = dense_model.encode([query], normalize_embeddings=True)[0].tolist()\n",
    "    sparse_vec = generate_sparse_embedding(query)\n",
    "    \n",
    "    # create search requests\n",
    "    # Fetch more candidates for individual searches\n",
    "    candidate_limit = max(limit * 10, 100) \n",
    "    expr = f\"publication_year >= {start_year} && publication_year <= {end_year}\"\n",
    "    dense_req = AnnSearchRequest(\n",
    "        data=[dense_vec],\n",
    "        anns_field=\"dense_vector\",\n",
    "        param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=candidate_limit,\n",
    "        expr=expr\n",
    "    )\n",
    "    sparse_req = AnnSearchRequest(\n",
    "        data=[sparse_vec],\n",
    "        anns_field=\"sparse_vector\",\n",
    "        param={\"metric_type\": \"IP\", \"params\": {\"drop_ratio_search\": 0.2}},\n",
    "        limit=candidate_limit,\n",
    "        expr=expr\n",
    "    )\n",
    "    \n",
    "    # perform search\n",
    "    collection.load()\n",
    "    res = collection.hybrid_search(\n",
    "        reqs=[dense_req, sparse_req],\n",
    "        rerank=WeightedRanker(alpha, 1-alpha), # alpha * Dense + (1-alpha) * Sparse\n",
    "        limit=limit,\n",
    "        output_fields=[\"arxiv_id\", \"publication_year\"]\n",
    "    )\n",
    "    \n",
    "    if not res:\n",
    "        return []\n",
    "    \n",
    "    # format and return results\n",
    "    results = [{\n",
    "        \"arxiv_id\": hit.entity.get(\"arxiv_id\"), \n",
    "        \"score\": hit.score, \n",
    "        \"year\": hit.entity.get(\"publication_year\")\n",
    "        } for hit in res[0]]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "query = \"Transformer Attention\"\n",
    "\n",
    "attention_id = '1706.03762' # arXiv for Attention Is All You Need\n",
    "print(f\"Querying: '{query}'\")\n",
    "output = search_papers(query, start_year=2010, end_year=2024, alpha=0.5, limit=50)\n",
    "\n",
    "# join output with original data\n",
    "display_cols = ['arxiv_id', 'title', 'published_date', 'score']\n",
    "results = output.merge(papers, on='arxiv_id')\n",
    "\n",
    "# Check rank of target paper\n",
    "if attention_id in results['arxiv_id'].values:\n",
    "    rank = results[results['arxiv_id'] == attention_id].index[0] + 1\n",
    "    score = results[results['arxiv_id'] == attention_id].score.values[0]\n",
    "    print(f\"Success! 'Attention Is All You Need' found at rank {rank} with score {score:.4f}\")\n",
    "else:\n",
    "    print(f\"Warning: 'Attention Is All You Need' ({attention_id}) not found in top {len(results)}.\")\n",
    "\n",
    "results[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2e461",
   "metadata": {},
   "source": [
    "## Concept Evolution\n",
    "\n",
    "Now instead of just ranking individual papers, we want to understand the associations with the concept and how they change over time. A single query like \"Transformer\" might refer to multiple distinct sub-concepts (e.g., \"Electrical Power\", \"Neural Networks\", \"Signal Processing\"). To do this, we will:\n",
    "\n",
    "1. Cluster the search results using their Dense vectors (semantic meaning), so the papers will be grouped into distinct sub-concepts\n",
    "2. Label each cluster using the Sparse vectors (Keywords). We will then aggregate the SPLADE vectors of papers in each cluster to find the most representative tokens\n",
    "3. Visualize the evolution of these clusters over time to see the change in relevance for these specific meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15afde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'Transformer Attention'...\n",
      "Analyzing concepts...\n",
      "Clustering 500 papers into 2 concepts...\n",
      "Generating cluster labels...\n",
      "\n",
      "Identified Concepts:\n",
      "Cluster 0 (198 papers): transform, methods, algorithm, models, data\n",
      "Cluster 1 (302 papers): transform, theory, quantum, experiment, transformation\n",
      "Generating cluster labels...\n",
      "\n",
      "Identified Concepts:\n",
      "Cluster 0 (198 papers): transform, methods, algorithm, models, data\n",
      "Cluster 1 (302 papers): transform, theory, quantum, experiment, transformation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Label-Attention Transformer with Geometrically...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Phase Conductor on Multi-layered Attentions fo...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>R-GAT: Cancer Document Classification Leveragi...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>Investigating Acoustic-Textual Emotional Incon...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>Syntax-based data augmentation for Hungarian-E...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022</td>\n",
       "      <td>HeadPosr: End-to-end Trainable Head Pose Estim...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019</td>\n",
       "      <td>An Attention Mechanism for Musical Instrument ...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019</td>\n",
       "      <td>Attention is not not Explanation</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>MSEG-VCUQ: Multimodal SEGmentation with Enhanc...</td>\n",
       "      <td>transform, methods, algorithm, models, data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title  \\\n",
       "0  2021  Label-Attention Transformer with Geometrically...   \n",
       "1  2017  Phase Conductor on Multi-layered Attentions fo...   \n",
       "2  2024  R-GAT: Cancer Document Classification Leveragi...   \n",
       "3  2024  Investigating Acoustic-Textual Emotional Incon...   \n",
       "4  2022  Syntax-based data augmentation for Hungarian-E...   \n",
       "5  2022  HeadPosr: End-to-end Trainable Head Pose Estim...   \n",
       "6  2017                          Attention Is All You Need   \n",
       "7  2019  An Attention Mechanism for Musical Instrument ...   \n",
       "8  2019                   Attention is not not Explanation   \n",
       "9  2024  MSEG-VCUQ: Multimodal SEGmentation with Enhanc...   \n",
       "\n",
       "                                 cluster_label  \n",
       "0  transform, methods, algorithm, models, data  \n",
       "1  transform, methods, algorithm, models, data  \n",
       "2  transform, methods, algorithm, models, data  \n",
       "3  transform, methods, algorithm, models, data  \n",
       "4  transform, methods, algorithm, models, data  \n",
       "5  transform, methods, algorithm, models, data  \n",
       "6  transform, methods, algorithm, models, data  \n",
       "7  transform, methods, algorithm, models, data  \n",
       "8  transform, methods, algorithm, models, data  \n",
       "9  transform, methods, algorithm, models, data  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_concept_evolution(search_results_df: pd.DataFrame, embeddings_df: pd.DataFrame, n_clusters) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Clusters search results into sub-concepts and tracks them over time.\n",
    "    \"\"\"\n",
    "    # merge with embeddings to get the vectors\n",
    "    # need dense vectors for clustering, sparse vectors for labeling\n",
    "    df = search_results_df.merge(embeddings_df[['arxiv_id', 'dense_vector', 'sparse_vector', 'title']], on='arxiv_id')\n",
    "    \n",
    "    if len(df) < n_clusters:\n",
    "        raise ValueError(f\"Not enough data points ({len(df)}) for {n_clusters} clusters.\")\n",
    "\n",
    "    # build clusters from dense vectors\n",
    "    print(f\"Clustering {len(df)} papers into {n_clusters} concepts...\")\n",
    "    matrix = np.stack(df['dense_vector'].values)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df['cluster'] = kmeans.fit_predict(matrix)\n",
    "    \n",
    "    # build cluster labels with sparse vectors\n",
    "    cluster_labels = {}\n",
    "    print(\"Generating cluster labels...\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster = df[df['cluster'] == cluster_id]\n",
    "        \n",
    "        # build map (token -> token's total weight in the cluster)\n",
    "        token_weights = defaultdict(float)\n",
    "        for sparse_vec in cluster['sparse_vector']:\n",
    "            for token_id, weight in sparse_vec.items():\n",
    "                token_weights[token_id] += weight\n",
    "                \n",
    "        # decode top tokens (i.e., tokens with largest weight)\n",
    "        top_tokens = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        decoded_tokens = [sparse_tokenizer.decode([token]) for token, _ in top_tokens]\n",
    "        \n",
    "        # use decoded tokens (i.e., sub-concepts) as labels\n",
    "        label = \", \".join(decoded_tokens)\n",
    "        cluster_labels[cluster_id] = label\n",
    "        \n",
    "    df['cluster_label'] = df['cluster'].map(cluster_labels)\n",
    "    return df, cluster_labels\n",
    "\n",
    "\n",
    "print(f\"Searching for '{query}'...\")\n",
    "concept_results = search_papers(query, limit=500, alpha=0.5) # use a larger limit\n",
    "\n",
    "print(\"Analyzing concepts...\")\n",
    "clustered_df, labels = analyze_concept_evolution(concept_results, embedded_papers, n_clusters = 2)\n",
    "\n",
    "print(\"\\nIdentified Concepts:\")\n",
    "for cid, label in labels.items():\n",
    "    count = len(clustered_df[clustered_df['cluster'] == cid])\n",
    "    print(f\"Cluster {cid} ({count} papers): {label}\")\n",
    "\n",
    "clustered_df[['year', 'title', 'cluster_label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94350c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Concepts=transform, theory, quantum, experiment, transformation<br>Year=%{x}<br>Number of Papers=%{y}<extra></extra>",
         "legendgroup": "transform, theory, quantum, experiment, transformation",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "transform, theory, quantum, experiment, transformation",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "xwfIB8kHygfLB8wHzQfOB88H0AfRB9IH0wfUB9UH1gfXB9gH2QfaB9sH3AfdB94H3wfgB+EH4gfjB+QH5QfmB+cH6AfpBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQUDCAcCAwYHDgkLBQcKCgcJCgoKDgkMCgwFDAcKCQwHEQw=",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Concepts=transform, methods, algorithm, models, data<br>Year=%{x}<br>Number of Papers=%{y}<extra></extra>",
         "legendgroup": "transform, methods, algorithm, models, data",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "transform, methods, algorithm, models, data",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": {
          "bdata": "yAfKB84HzwfQB9EH0gfTB9QH1QfWB9cH2AfZB9oH2wfcB90H3gffB+AH4QfiB+MH5AflB+YH5wfoB+kH",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQEEBAUBBAEEBAQGAgMEBgYDCQsFCgcOCwgTCgsU",
          "dtype": "i1"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "stack",
        "legend": {
         "title": {
          "text": "Concepts"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Associations with 'Transformer Attention' Over Time"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of Papers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_concept_shifts(df):\n",
    "    counts = df.groupby(['year', 'cluster_label']).size().reset_index(name='count').sort_values('year')\n",
    "    fig = px.bar(\n",
    "        counts, \n",
    "        x=\"year\", \n",
    "        y=\"count\", \n",
    "        color=\"cluster_label\",\n",
    "        title=f\"Associations with '{query}' Over Time\",\n",
    "        labels={\"count\": \"Number of Papers\", \"year\": \"Year\", \"cluster_label\": \"Concepts\"},\n",
    "        barmode='stack'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_concept_shifts(clustered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific-concept-evolution-tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
