
\documentclass[sigconf,nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% % Do not print ACM Reference Format
\settopmatter{printacmref=false}

%% % Location of your graphics files for figures, here a sub-folder to the main project folder
\graphicspath{{./images/}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Scientific Concept Evolution Tracker}

\acmConference[CSC2508: Advanced Data Systems]{CSC2508}{Fall 2025}{University of Toronto}

%%
\author{Nic Bolton}
\affiliation{%
  \institution{University of Toronto}
  \streetaddress{27 King's College Circle}
  \city{Toronto}
\country{Canada}}
\email{nic@cs.toronto.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{N. Bolton}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  With the sheer volume of research being published, the history and context of how scientific ideas evolve are often difficult to visualize through the noise.
  Terminology in science can be dynamic---the semantic meaning of terms such as ``neural networks'', ``entropy'', or ``plasma'' shift significantly over decades as new research subfields emerge.
  Traditional information retrieval systems and static vector databases index semantic meanings as fixed points in high-dimensional space, which flattens the temporal dimension and hides the evolutionary history of these concepts.
  This report introduces the Scientific Concept Evolution Tracker (SCET), a comprehensive system designed to ingest, index, and analyze large-scale scientific corpora to quantify this semantic drift.
  SCET is built for scale with PostgreSQL for storing metadata and Milvus for embeddings.
  We introduce a methodology that combines unsupervised clustering (K-Means) with temporal segmentation (Decision Tree Regression) to automatically identify distinct ``eras'' of a concept's life cycle.
  We demonstrate the system's capabilities through case studies, such as the divergence of ``Transformer'' from electrical engineering to natural language processing, and provide a quantitative analysis of system performance on a dataset sourced from arXiv.
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Science is a cumulative endeavor, yet the language of science is fluid.
A core challenge in bibliometrics and the ``Science of Science'' is understanding how scientific consensus and terminology evolve.
For a researcher entering a new field, understanding the historical context of a term is as critical as understanding its current definition.
For example, a query for ``Attention'' in 2005 would yield results dominated by cognitive psychology and neurobiology.
The same query in the late 2010s and early 2020s is overwhelmingly dominated by field of machine learning.
This is the phenomenon that we are interested in, that is, being able to quantify how a concept's relevance or association with specific fields change over time.

The introduction and combination of Large Language Models (LLMs) and vector databases have revolutionized semantic search.
By representing text as dense vectors in a high-dimensional space, we can capture semantic similarity beyond simple keyword matching.
However, most vector search implementations treat the document corpus as a static snapshot.
They are designed to answer the question, ``What is semantically similar to this query now?'' rather than ``How has the meaning of this query changed over time?''.

This project addresses the following question.
How can we design a scalable vector database system that can quantify the semantic evolution of scientific concepts and identify pivotal publications that drive these shifts?

SCET was developed to answer this question.
It consists of a pipeline that:

\begin{enumerate}
  \item Ingests and indexes scientific abstracts using a hybrid embedding strategy
  \item Retrieves context-aware results using a weighted hybrid search
  \item Clusters results into sub-concepts associated with a given query
  \item Produces a set of time periods that represent ``stable'' eras of a sub-concepts association/relevancy
  \item Identifies papers that are ``pivotal'' to the shift into these eras
\end{enumerate}

The remainder of this paper is organized as follows.
We first review the background and related work in NLP and bibliometrics.
We then discuss the details of the methodology and system architecture, followed by a presentation of the experimental results and case studies.
We conclude by discussing limitations and future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background and Related Work}

The problem to solve (tracking the evolution of scientific concepts) sits at the intersection of Natural Language Processing (NLP), Information Retrieval (IR), and the ``Science of Science''.
This section reviews the historical progression of these fields and the specific technologies that enable SCET.

\subsection{The Evolution of Information Retrieval}

The field of Information Retrieval has evolved through several distinct paradigms.
An overview of the history is listed below.
It is important to note that although these systems perform well within their own goals, they all lack the ability to capture semantic and contextual meaning within terms.

\subsubsection{Boolean Logic}
The earliest IR systems relied on semantics used within set theory, where queries were built with operators such as AND, OR, NOT.
These systems could make retrievals at a high level of precision by using Inverted Indices.
These work as a key-value store, that is, each term is a key that is associated with a list of pages that contain the term.
This results in a very fast and precise system \cite{ir:boolean}.

\subsubsection{TF-IDF}
Term Frequency Inverse Document Frequency (TF-IDF) introduced the concept of weighting the importance of words.
This system involves calculating a score for a term, derived by balancing how frequently a term appears in a specific document (TF) against how rarely the term appears across the entire collection of documents (IDF).
Consequently, these scores often favour the terms that best characterize the topics involved in a collection of documents. \cite{ir:manning}.

\subsubsection{Vector Space Model}
The Vector Space Model (VSM) generalized the idea of weighting terms by representing documents as vectors in a multi-dimensional space, where each dimension corresponds to a distinct term in the corpus.
In this model, the relevance of a document to a query is measured by the similarity between their respective vectors.
The similarity measurement is often done using Cosine Similarity
\begin{equation}
  \cos (\boldsymbol{q}, \boldsymbol{d}) = \frac{\boldsymbol{q} \cdot \boldsymbol{d}}{\Vert \boldsymbol{q} \Vert \Vert \boldsymbol{d} \Vert}
\end{equation}
for TF-IDF weights of the query $\boldsymbol{q}$ and the document $\boldsymbol{d}$.

This allows for ranked retrieval results based on the angle between vectors, rather than a binary inclusion/exclusion.
However, traditional VSMs treat terms as orthogonal dimensions, meaning they cannot capture semantic relationships between synonyms (e.g., ``car'' and ``automobile'') without explicit expansion \cite{ir:manning}.

\subsubsection{BM25}
Okapi Best Matching 25 (BM25) represents a significant evolution in probabilistic information retrieval.
While sharing similarities with TF-IDF, BM25 introduces two critical improvements: term saturation and document length normalization.
Unlike TF-IDF, where the score increases linearly with term frequency, BM25 applies a saturation function so that the value of a term diminishes as it is repeated (thus, preventing long repetitions of keywords from dominating results).
Additionally, it penalizes long documents (which naturally contain more terms) to prevent them from unfairly dominating search results.
BM25 remains a strong baseline for lexical search tasks today.\cite{ir:bm25}.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
  Acknowledgements go here. Delete enclosing begin/end markers if there are no acknowledgements.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%%
\end{document}
\endinput
%%
%% End of file `main.tex'.
