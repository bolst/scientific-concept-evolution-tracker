
\documentclass[sigconf,nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% % Do not print ACM Reference Format
\settopmatter{printacmref=false}

%% % Location of your graphics files for figures, here a sub-folder to the main project folder
\graphicspath{{./images/}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Scientific Concept Evolution Tracker}

\acmConference[CSC2508: Advanced Data Systems]{CSC2508}{Fall 2025}{University of Toronto}

%%
\author{Nic Bolton}
\affiliation{%
  \institution{University of Toronto}
  \streetaddress{27 King's College Circle}
  \city{Toronto}
\country{Canada}}
\email{nic@cs.toronto.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{N. Bolton}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  With the sheer volume of research being published, the history and context of how scientific ideas evolve are often difficult to visualize through the noise.
  Terminology in science can be dynamic---the semantic meaning of terms such as ``neural networks'', ``entropy'', or ``plasma'' shift significantly over decades as new research subfields emerge.
  Traditional information retrieval systems and static vector databases index semantic meanings as fixed points in high-dimensional space, which flattens the temporal dimension and hides the evolutionary history of these concepts.
  This report introduces the Scientific Concept Evolution Tracker (SCET), a comprehensive system designed to ingest, index, and analyze large-scale scientific corpora to quantify this semantic drift.
  SCET is built for scale with PostgreSQL for storing metadata and Milvus for embeddings.
  We introduce a methodology that combines unsupervised clustering (K-Means) with temporal segmentation (Decision Tree Regression) to automatically identify distinct ``eras'' of a concept's life cycle.
  We demonstrate the system's capabilities through case studies, such as the divergence of ``Transformer'' from electrical engineering to natural language processing, and provide a quantitative analysis of system performance on a dataset sourced from arXiv.
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Science is a cumulative endeavor, yet the language of science is fluid.
A core challenge in bibliometrics and the ``Science of Science'' is understanding how scientific consensus and terminology evolve.
For a researcher entering a new field, understanding the historical context of a term is as critical as understanding its current definition.
For example, a query for ``Attention'' in 2005 would yield results dominated by cognitive psychology and neurobiology.
The same query in the late 2010s and early 2020s is overwhelmingly dominated by field of machine learning.
This is the phenomenon that we are interested in, that is, being able to quantify how a concept's relevance or association with specific fields change over time.

The introduction and combination of Large Language Models (LLMs) and vector databases have revolutionized semantic search.
By representing text as dense vectors in a high-dimensional space, we can capture semantic similarity beyond simple keyword matching.
However, most vector search implementations treat the document corpus as a static snapshot.
They are designed to answer the question, ``What is semantically similar to this query now?'' rather than ``How has the meaning of this query changed over time?''.

This project addresses the following question.
How can we design a scalable vector database system that can quantify the semantic evolution of scientific concepts and identify pivotal publications that drive these shifts?

SCET was developed to answer this question.
It consists of a pipeline that:

\begin{enumerate}
  \item Ingests and indexes scientific abstracts using a hybrid embedding strategy
  \item Retrieves context-aware results using a weighted hybrid search
  \item Clusters results into sub-concepts associated with a given query
  \item Produces a set of time periods that represent ``stable'' eras of a sub-concepts association/relevancy
  \item Identifies papers that are ``pivotal'' to the shift into these eras
\end{enumerate}

The remainder of this paper is organized as follows.
We first review the background and related work in NLP and bibliometrics.
We then discuss the details of the methodology and system architecture, followed by a presentation of the experimental results and case studies.
We conclude by discussing limitations and future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background and Related Work}

The problem to solve (tracking the evolution of scientific concepts) sits at the intersection of Natural Language Processing (NLP), Information Retrieval (IR), and the ``Science of Science''.
This section reviews the historical progression of these fields and the specific technologies that enable SCET.

\subsection{The Evolution of Information Retrieval}

The field of Information Retrieval has evolved through several distinct paradigms.
An overview of the history is listed below.
It is important to note that although these systems perform well within their own goals, they all lack the ability to capture semantic and contextual meaning within terms.

\subsubsection{Boolean Logic}
The earliest IR systems relied on semantics used within set theory, where queries were built with operators such as AND, OR, NOT.
These systems could make retrievals at a high level of precision by using Inverted Indices.
These work as a key-value store, that is, each term is a key that is associated with a list of pages that contain the term.
This results in a very fast and precise system \cite{ir:boolean}.

\subsubsection{TF-IDF}
Term Frequency Inverse Document Frequency (TF-IDF) introduced the concept of weighting the importance of words.
This system involves calculating a score for a term, derived by balancing how frequently a term appears in a specific document (TF) against how rarely the term appears across the entire collection of documents (IDF).
Consequently, these scores often favour the terms that best characterize the topics involved in a collection of documents. \cite{ir:manning}.

\subsubsection{Vector Space Model}
The Vector Space Model (VSM) generalized the idea of weighting terms by representing documents as vectors in a multi-dimensional space, where each dimension corresponds to a distinct term in the corpus.
In this model, the relevance of a document to a query is measured by the similarity between their respective vectors.
The similarity measurement is often done using Cosine Similarity
\begin{equation}
  \cos (\boldsymbol{q}, \boldsymbol{d}) = \frac{\boldsymbol{q} \cdot \boldsymbol{d}}{\Vert \boldsymbol{q} \Vert \Vert \boldsymbol{d} \Vert}
\end{equation}
for TF-IDF weights of the query $\boldsymbol{q}$ and the document $\boldsymbol{d}$.

This allows for ranked retrieval results based on the angle between vectors, rather than a binary inclusion/exclusion.
However, traditional VSMs treat terms as orthogonal dimensions, meaning they cannot capture semantic relationships between synonyms (for example, car and automobile) without explicit expansion \cite{ir:manning}.

\subsubsection{BM25}
Okapi Best Matching 25 (BM25) represents a significant evolution in probabilistic information retrieval.
While sharing similarities with TF-IDF, BM25 introduces two critical improvements: term saturation and document length normalization.
Unlike TF-IDF, where the score increases linearly with term frequency, BM25 applies a saturation function so that the value of a term diminishes as it is repeated (thus, preventing long repetitions of keywords from dominating results).
Additionally, it penalizes long documents (which naturally contain more terms) to prevent them from unfairly dominating search results.
BM25 remains a strong baseline for lexical search tasks today.\cite{ir:bm25}.

\subsection{Evolution of NLP Representations}

The representation of text is fundamental to our ability to measure drift.

\subsubsection{Static Embeddings}
Word2Vec \cite{word2vec} and GloVe \cite{glove} introduced distributed representations. They rely on the distributional hypothesis: ``a word is characterized by the company it keeps''.
These models map discrete tokens to dense vectors in a continuous space, capturing syntactic and semantic regularities.
The vectors that are assigned to these tokens (words) are derived so that they are in close proximity to other similar words (for example, the vectors for ``dog'' and ``puppy'' are very close).
However, these models are static, meaning they assign a fixed vector to each word type regardless of context.
For example, a vector would be generated for the word ``bank'', but would be indifferent whether it was used in the context of river banks or piggy banks.
This imposes a significant limitation for scientific text, where acronyms and terms often have distinct or field-specific definitions that cannot be derived via static representation.

\subsubsection{Contextual Embeddings}
ELMo \cite{elmo} and BERT \cite{bert} introduced dynamic embeddings to address the limitations of static models.
The Transformer \cite{attention} architecture's self-attention mechanism allows the representation of a token to be a function of its surrounding context.
BERT (Bidirectional Encoder Representations from Transformers) pre-trains on a masked language modeling objective, allowing it to learn deep bidirectional representations.
Unlike static embeddings, BERT generates a representation for a token that is conditioned on the entire input sequence.
This allows a word such as ``bank'' to become associated with the context it was used in---if it sees ``river'' nearby, then it can identify river bank as the likely association.
This concept is useful for our problem, as it allows for distinguishing ``Attention'' (psychology) from ``Attention'' (machine learning) in a given corpus.

\subsubsection{Domain-Specific Models}
Although contextual embedding models such as BERT are great drivers for advancing the field of NLP, they often underperform on domain-specific corpora such as scientific text.
This is because they are trained on corpora such as Wikipedia, which differs significantly in vocabulary and syntax from scientific literature.
SciBERT \cite{scibert} addresses this by pre-training BERT on a large corpus of scientific papers.
SPECTER \cite{specter2} took this further by leveraging a unique feature of research: citations.
They used citation graphs as a signal for semantic similarity, which groups papers based on how related they are as opposed to just textual overlap.
It uses a triplet loss objective:
\begin{equation*}
  \mathcal{L} = \max \left\{ d(q, p^+) - d(q, p^-) + m, 0 \right\}
\end{equation*}
where $d$ is a distance function, $q$ is a query paper, $p^+$ is a cited paper, $p^-$ is a non-cited paper (but may be cited by $p^+$), and $m$ is the loss margin hyperparameter.
By training the model to pull cited papers closer in vector space and push unrelated papers apart, 2 learns embeddings that reflect the functional relationships between papers.

\subsection{Vector Database Indexing}

Searching a dataset of millions of high-dimensional vectors is computationally prohibitive using brute force ($O(N)$).
SCET relies on Approximate Nearest Neighbor (ANN) algorithms.

\subsubsection{Inverted File Index}
The Inverted File Index (IVF) works by partitioning the vector space into Voronoi cells, where every document vector is then assigned to its nearest centroid.
Now upon search, the system can extract a subset of the vectors by comparing the query vector to the centroids.
A benefit to IVF is its low memory footprint \cite{ir:manning}.

\subsubsection{Hierarchical Navigable Small World}
While partition-based methods like IVF offer memory efficiency, graph-based approaches currently provide the superior trade-off between latency and recall.
Hierarchical Navigable Small World (HNSW) structures data into a multi-layered graph hierarchy inspired by Skip Lists and the ``small world'' phenomenon \cite{small-world}.
The upper layers consist of sparse, long-range links that allow the search algorithm to traverse the vector space rapidly, effectively zooming in on the target region.
Once the coarse location is identified, the search descends to lower, denser layers for fine-grained greedy traversal to locate the nearest neighbors.
Although HNSW requires higher memory overhead to store the graph connectivity compared to quantization methods, it is robust against the curse of dimensionality and does not require the training phases of clustering approaches \cite{hnsw}.

\subsection{Semantic Drift Analysis}

Semantic drift (or semantic change) is the study of change with respect to the meaning of words, specifically the evolution of how a word is used.
For example, the word \textit{awful} originally meant to inspire wonder or fear, and hence impressive.
Today, it is used to describe something that is regarded as very bad.

\subsubsection{Alignment}
Since embedding models are initialized randomly, the vector spaces for different time periods often end up rotated or flipped relative to each other.
Orthogonal Procrustes Analysis is used to fix this, by mathematically rotating the vector space of one time period to align it with the next.
By locking the two maps together, it ensures that if a word's position changes, it represents a genuine shift in meaning rather than a side effect of the model's random initialization \cite{procrustes} .

\subsubsection{Dynamic Word Embeddings}
While alignment-based methods rely on independent training of temporal slices, dynamic probabilistic models treat the evolution of semantic meaning as a continuous latent variable process.
First introduced by Balmer and Mandt \cite{dynamicwords}, their process involves training a single global model where the embedding of a term at time $t$ is conditioned on its embedding at time $t-1$, modeled as a Gaussian Random Walk.
This process ensures that meanings shift gradually rather than abruptly.
By using data from surrounding time periods, these models work much better for rare terms.
This eliminates the random noise often seen in year-by-year training, resulting in a clearer, smoother path of how a concept has changed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}

\subsection{System Architecture and Data Pipeline}

ArXiv has a public Google Cloud Storage Bucket that includes the metadata of approximately 4 million publications.
For this project, we utilize a snapshotted version of this metadata, which is available on Kaggle \cite{kaggle:arxiv} and is provided as a 3.8GB JSON file.

\subsubsection{Data Ingestion and Relational Storage}
Due to the size of the dataset, loading the entire file into memory was infeasible.
We resolved this by processing the file in batches.
For each publication we extract its unique arXiv ID along with the title, abstract, primary category, publication date, and DOI.

This structured metadata is stored in a PostgreSQL \cite{postgres} database.
PostgreSQL serves as the source of truth for document metadata and enables efficient filtering based on structured fields (for example, retrieving all papers from 2015 in the \textit{cs.AI} category).
The database schema is designed to handle the metadata efficiently, with the arXiv ID serving as the primary key that links the relational data in PostgreSQL to the embeddings in the vector database.
This separation of concerns allows each system to be optimized for its specific access patterns.

\subsubsection{Vector Storage Layer (Milvus)}
We deploy Milvus Standalone v2.6.6 \cite{milvus} as our vector database.
Milvus was selected for its cloud-native architecture, which separates storage from compute, allowing us to scale query nodes independently of data volume.
To optimize for our specific access patterns (which heavily rely on filtering by time and scientific domain) we implement a custom partitioning strategy.
We define a partition key derived from a hash of the publication year and primary category.
This allows the query engine to prune irrelevant partitions during search, significantly reducing the search space for temporal queries.
For example, a query for \textit{Machine Learning Papers in 2015} only needs to scan a small fraction of the total index, rather than the entire collection of 4 million vectors. The collection schema is detailed in Table \ref{tab:schema}.

\begin{table*}
  \caption{Milvus Collection Schema}
  \label{tab:schema}
  \begin{tabular}{ccl}
    \toprule
    Field Name & Data Type & Description \\
    \midrule
    \texttt{paper\_id} & Int64 & Primary Key (64-bit hash of arXiv ID) \\
    \texttt{arxiv\_id} & VarChar(32) & ArXiv ID (for debugging) \\
    \texttt{dense\_vector} & FloatVector (768) & Semantic embedding (SPECTER2) \\
    \texttt{sparse\_vector} & SparseFloatVector & Lexical embedding (SPLADE) \\
    \texttt{publication\_year} & Int16 & Scalar field for temporal filtering \\
    \texttt{partition\_key} & Int64 & Hash of publication year and category \\
    \bottomrule
  \end{tabular}
\end{table*}

We utilize Hierarchical Navigable Small World (HNSW) indexes for dense vectors.
We configure the index with $M=16$ (the number of bi-directional links per node) and $efConstruction=200$ (the size of the dynamic candidate list during build).
This configuration was selected to maximize recall while maintaining interactive query latency at the cost of higher memory consumption, compared to quantization-based indexes like Inverted File with Product Quantization \cite{ivfpq}.
For the sparse vectors, we use a Sparse Inverted Index, which is analogous to the inverted indices used in traditional search engines but optimized for floating-point weights.

\subsubsection{Deployment and Infrastructure}
The entire SCET system is containerized using Docker \cite{docker} to ensure reproducibility and ease of deployment.
We utilize Docker Compose to orchestrate the necessary services, including Milvus Standalone as the core vector database engine, etcd \cite{etcd} for metadata management and service discovery, MinIO \cite{minio} as an S3-compatible object storage service for persisting vector data, and PostgreSQL for storing structured paper metadata.

The system is deployed on a Linux-based home lab server.
The hardware consists of an Intel Xeon E5-1620 v3 CPU (8 cores @ 3.50 GHz), 16 GB of RAM, and a Seagate BarraCuda 20TB HDD.
Resource limits are enforced via Docker Compose, with Milvus allocated 8GB of RAM, etcd 4GB, and MinIO 2GB.

\subsection{Hybrid Embedding Strategy}
SCET employs a \textit{Retrieve-then-Rerank} strategy using a hybrid score to balance semantic understanding with lexical precision.
Pure lexical search often suffers from the \textit{vocabulary mismatch problem} (or lexical gap), where it fails to retrieve relevant documents that use synonyms (for example, missing a paper on ``automobiles'' when querying for ``cars'') \cite{lexicalgap}.
Conversely, pure dense retrieval can lack \textit{lexical precision}, retrieving semantically related documents that miss specific query terms (for example, retrieving a paper about recurrent neural networks when the user specifically asked for LSTMs).
We use a hybrid approach so that both of these issues can be addressed.

\subsubsection{Dense Embedding (Semantic)}
We utilize SPECTER2 \cite{specter2base}, which succeeds the discussed SPECTER model that is pre-trained on scientific citations.
The input to the model is a concatenation of the paper's title and abstract, separated by SPECTER2's provided separator token: \texttt{Title[SEP]Abstract}.
This generates a 768-dimensional dense vector that captures the high-level semantic intent of the paper.
This explicitly encodes a ``scientific relatedness'' into the vector space: papers are encoded as queries/candidates that are intended for use in link predictions or nearest neighbor searches.

\subsubsection{Sparse Embedding (Lexical)}
To prevent the loss of specific keyword information, we also generate sparse embeddings using SPLADE (Sparse Lexical and Expansion Model) \cite{splade}.
SPLADE works by taking sentences or paragraphs as input and mapping them to a 30522-dimensional vector space.
Unlike traditional bag-of-words models (such as TF-IDF) which only assigns weights to terms present in the document, SPLADE performs \textit{term expansion}.
It uses the BERT Masked Language Model to predict relevant terms that do not appear in the text but are semantically related, assigning them non-zero weights.
Mathematically, the weight $w_{ij}$ for token $j$ in document $i$ is calculated by aggregating the log-saturated attention weights across all input tokens.
This functions as a ``learned'' inverted index, providing the precision of keyword matching with the recall of semantic expansion.
This effectively allows us to get the benefits of both BM25 and dense retrieval \cite{splade:original}.

\subsubsection{Hybrid Scoring}
Retrieval is performed by executing two parallel searches: an ANN search on the dense index along with a term-matching search on the sparse index.
A common challenge in hybrid search is the \textit{rank fusion problem}, that is, if we only retrieve the top $k$ results from each index, a document that is relevant in both but not top-ranked in either might be discarded.
To address this, we fetch a candidate pool significantly larger than the final limit (for example, $10*k$) from both indices.
We then normalize the scores (since Cosine Similarity scores are bound to the interval $[-1, 1]$, and Inner Product is unbounded) and combine them using a weighted sum:
\begin{equation}
  S_{hybrid} = \alpha \cdot S_{dense} + (1 - \alpha) \cdot S_{sparse}
\end{equation}
where $\alpha$ is a hyperparameter controlling the trade-off between semantic and lexical relevance.
We default to $\alpha=0.5$, giving equal weight to conceptual similarity and keyword precision.

\subsection{Concept Clustering Algorithm}
To identify the distinct meanings (sub-concepts) associated with a query term, we perform clustering on the retrieved dense vectors.
The underlying assumption is the Distributional Hypothesis applied to vector space.
Papers discussing the same sub-concept (for example, ``Corona'' as a virus) will form a dense cluster that is spatially separated from papers discussing a different sub-concept (for example, ``Corona'' as a solar feature).

Given a set of search results for a query $Q$, we extract their dense vectors $V_Q$.
We then apply $k$-Means clustering to partition $V_Q$ into $k$ clusters.
The optimal number of clusters $k$ is determined dynamically for each query by maximizing the Silhouette Score \footnote{The Silhouette Score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation), which is a metric we can use for cluster quality without the need for the actual labels.} over a range such as $[2,8]$.

To interpret these clusters, we leverage the sparse vectors.
Since SPLADE vectors represent the ``lexical essence'' of a document, we can generate a descriptive label for a cluster by summing the sparse vectors of all papers within it.
The tokens with the highest summed weights represent the most discriminative keywords for that group.
For example, one cluster for ``Transformer'' might yield top tokens ``voltage, power, current'', while another yields ``language, sequence, attention''.
This serves as a convenient source for user interpretability.

\subsection{Era Detection}
Once sub-concepts are identified, we aim to detect their temporal ``eras'', that is, periods of stable activity or relevance.
We treat the yearly publication count of a sub-concept as a time series signal.
A naive approach might look for peaks or valleys, but publication data is noisy.
Instead, we employ a Decision Tree Regressor to approximate this signal with a step function.
By fitting a decision tree to the annual publication count with a limited depth, the algorithm naturally finds the optimal split points that minimize the variance within each leaf.
These split points (thresholds) correspond to the temporal boundaries where the trend significantly changes.
The leaf nodes of the tree represent the stable eras (for example, 2000--2012: low activity, 2013--2023: high activity).
This method is non-parametric and robust to outliers, making it well-suited for bibliometric data.

\subsection{Identifying Pivotal Papers}
For each identified era, we seek to find the papers that best represent that specific semantic context during that time.
Simply selecting the most cited papers would bias the results towards older, established work.
Instead, we query the system for the top $N$ papers within the specific time window of the era, ranked by their hybrid score relative to the original query.
These papers serve as the ``anchors'' for understanding how the concept was defined and used during that period.
By surfacing these pivotal papers, SCET provides a curated list of papers that guides the user through the history of the concept, highlighting the seminal works that defined each era.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Results}

\subsection{System Performance Benchmarks}
\subsubsection{Ingestion Scalability}
\subsubsection{Query Latency}

\subsection{Case Studies}
\subsubsection{Transformer}
\subsubsection{Corona}

\subsection{Ablation Study: Hybrid vs. Dense-Only}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

\subsection{The Time Machine Effect}

\subsection{Scalability vs. Depth Trade-off}

\subsection{Limitations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
\begin{acks}
  We extend our gratitude to Dr. Nick Koudas for his guidance and support throughout CSC2508 Advanced Data Systems.
\end{acks}
%%

%%
\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}
%%

%%
\end{document}
\endinput
%%
