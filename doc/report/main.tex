
\documentclass[sigconf,nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% % Do not print ACM Reference Format
\settopmatter{printacmref=false}

%% % Location of your graphics files for figures, here a sub-folder to the main project folder
\graphicspath{{./images/}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Scientific Concept Evolution Tracker}

\acmConference[CSC2508: Advanced Data Systems]{CSC2508}{Fall 2025}{University of Toronto}

%%
\author{Nic Bolton}
\affiliation{%
  \institution{University of Toronto}
  \streetaddress{27 King's College Circle}
  \city{Toronto}
\country{Canada}}
\email{nic@cs.toronto.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{N. Bolton}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  With the sheer volume of research being published, the history and context of how scientific ideas evolve are often difficult to visualize through the noise.
  Terminology in science can be dynamic---the semantic meaning of terms such as ``neural networks'', ``entropy'', or ``plasma'' shift significantly over decades as new research subfields emerge.
  Traditional information retrieval systems and static vector databases index semantic meanings as fixed points in high-dimensional space, which flattens the temporal dimension and hides the evolutionary history of these concepts.
  This report introduces the Scientific Concept Evolution Tracker (SCET), a comprehensive system designed to ingest, index, and analyze large-scale scientific corpora to quantify this semantic drift.
  SCET is built for scale with PostgreSQL for storing metadata and Milvus for embeddings.
  We introduce a methodology that combines unsupervised clustering (K-Means) with temporal segmentation (Decision Tree Regression) to automatically identify distinct ``eras'' of a concept's life cycle.
  We demonstrate the system's capabilities through case studies, such as the divergence of ``Transformer'' from electrical engineering to natural language processing, and provide a quantitative analysis of system performance on a dataset sourced from arXiv.
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Science is a cumulative endeavor, yet the language of science is fluid.
A core challenge in bibliometrics and the ``Science of Science'' is understanding how scientific consensus and terminology evolve.
For a researcher entering a new field, understanding the historical context of a term is as critical as understanding its current definition.
For example, a query for ``Attention'' in 2005 would yield results dominated by cognitive psychology and neurobiology.
The same query in the late 2010s and early 2020s is overwhelmingly dominated by field of machine learning.
This is the phenomenon that we are interested in, that is, being able to quantify how a concept's relevance or association with specific fields change over time.

The introduction and combination of Large Language Models (LLMs) and vector databases have revolutionized semantic search.
By representing text as dense vectors in a high-dimensional space, we can capture semantic similarity beyond simple keyword matching.
However, most vector search implementations treat the document corpus as a static snapshot.
They are designed to answer the question, ``What is semantically similar to this query now?'' rather than ``How has the meaning of this query changed over time?''.

This project addresses the following question.
How can we design a scalable vector database system that can quantify the semantic evolution of scientific concepts and identify pivotal publications that drive these shifts?

SCET was developed to answer this question.
It consists of a pipeline that:

\begin{enumerate}
  \item Ingests and indexes scientific abstracts using a hybrid embedding strategy
  \item Retrieves context-aware results using a weighted hybrid search
  \item Clusters results into sub-concepts associated with a given query
  \item Produces a set of time periods that represent ``stable'' eras of a sub-concepts association/relevancy
  \item Identifies papers that are ``pivotal'' to the shift into these eras
\end{enumerate}

The remainder of this paper is organized as follows.
We first review the background and related work in NLP and bibliometrics.
We then discuss the details of the methodology and system architecture, followed by a presentation of the experimental results and case studies.
We conclude by discussing limitations and future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background and Related Work}

The problem to solve (tracking the evolution of scientific concepts) sits at the intersection of Natural Language Processing (NLP), Information Retrieval (IR), and the ``Science of Science''.
This section reviews the historical progression of these fields and the specific technologies that enable SCET.

\subsection{The Evolution of Information Retrieval}

The field of Information Retrieval has evolved through several distinct paradigms.
An overview of the history is listed below.
It is important to note that although these systems perform well within their own goals, they all lack the ability to capture semantic and contextual meaning within terms.

\subsubsection{Boolean Logic}
The earliest IR systems relied on semantics used within set theory, where queries were built with operators such as AND, OR, NOT.
These systems could make retrievals at a high level of precision by using Inverted Indices.
These work as a key-value store, that is, each term is a key that is associated with a list of pages that contain the term.
This results in a very fast and precise system \cite{ir:boolean}.

\subsubsection{TF-IDF}
Term Frequency Inverse Document Frequency (TF-IDF) introduced the concept of weighting the importance of words.
This system involves calculating a score for a term, derived by balancing how frequently a term appears in a specific document (TF) against how rarely the term appears across the entire collection of documents (IDF).
Consequently, these scores often favour the terms that best characterize the topics involved in a collection of documents. \cite{ir:manning}.

\subsubsection{Vector Space Model}
The Vector Space Model (VSM) generalized the idea of weighting terms by representing documents as vectors in a multi-dimensional space, where each dimension corresponds to a distinct term in the corpus.
In this model, the relevance of a document to a query is measured by the similarity between their respective vectors.
The similarity measurement is often done using Cosine Similarity
\begin{equation}
  \cos (\boldsymbol{q}, \boldsymbol{d}) = \frac{\boldsymbol{q} \cdot \boldsymbol{d}}{\Vert \boldsymbol{q} \Vert \Vert \boldsymbol{d} \Vert}
\end{equation}
for TF-IDF weights of the query $\boldsymbol{q}$ and the document $\boldsymbol{d}$.

This allows for ranked retrieval results based on the angle between vectors, rather than a binary inclusion/exclusion.
However, traditional VSMs treat terms as orthogonal dimensions, meaning they cannot capture semantic relationships between synonyms (e.g., car and automobile) without explicit expansion \cite{ir:manning}.

\subsubsection{BM25}
Okapi Best Matching 25 (BM25) represents a significant evolution in probabilistic information retrieval.
While sharing similarities with TF-IDF, BM25 introduces two critical improvements: term saturation and document length normalization.
Unlike TF-IDF, where the score increases linearly with term frequency, BM25 applies a saturation function so that the value of a term diminishes as it is repeated (thus, preventing long repetitions of keywords from dominating results).
Additionally, it penalizes long documents (which naturally contain more terms) to prevent them from unfairly dominating search results.
BM25 remains a strong baseline for lexical search tasks today.\cite{ir:bm25}.

\subsection{Evolution of NLP Representations}

The representation of text is fundamental to our ability to measure drift.

\subsubsection{Static Embeddings}
Word2Vec \cite{word2vec} and GloVe \cite{glove} introduced distributed representations. They rely on the distributional hypothesis: ``a word is characterized by the company it keeps''.
These models map discrete tokens to dense vectors in a continuous space, capturing syntactic and semantic regularities.
The vectors that are assigned to these tokens (words) are derived so that they are in close proximity to other similar words (for example, the vectors for ``dog'' and ``puppy'' are very close).
However, these models are static, meaning they assign a fixed vector to each word type regardless of context.
For example, a vector would be generated for the word ``bank'', but would be indifferent whether it was used in the context of river banks or piggy banks.
This imposes a significant limitation for scientific text, where acronyms and terms often have distinct or field-specific definitions that cannot be derived via static representation.

\subsubsection{Contextual Embeddings}
ELMo \cite{elmo} and BERT \cite{bert} introduced dynamic embeddings to address the limitations of static models.
The Transformer \cite{attention} architecture's self-attention mechanism allows the representation of a token to be a function of its surrounding context.
BERT (Bidirectional Encoder Representations from Transformers) pre-trains on a masked language modeling objective, allowing it to learn deep bidirectional representations.
Unlike static embeddings, BERT generates a representation for a token that is conditioned on the entire input sequence.
This allows a word such as ``bank'' to become associated with the context it was used in---if it sees ``river'' nearby, then it can identify river bank as the likely association.
This concept is useful for our problem, as it allows for distinguishing ``Attention'' (psychology) from ``Attention'' (machine learning) in a given corpus.

\subsubsection{Domain-Specific Models}
Although contextual embedding models such as BERT are great drivers for advancing the field of NLP, they often underperform on domain-specific corpora such as scientific text.
This is because they are trained on corpora such as Wikipedia, which differs significantly in vocabulary and syntax from scientific literature.
SciBERT \cite{scibert} addresses this by pre-training BERT on a large corpus of scientific papers.
SPECTER \cite{specter2} took this further by leveraging a unique feature of research: citations.
They used citation graphs as a signal for semantic similarity, which groups papers based on how related they are as opposed to just textual overlap.
It uses a triplet loss objective:
\begin{equation*}
  \mathcal{L} = \max \left\{ d(q, p^+) - d(q, p^-) + m, 0 \right\}
\end{equation*}
where $d$ is a distance function, $q$ is a query paper, $p^+$ is a cited paper, $p^-$ is a non-cited paper (but may be cited by $p^+$), and $m$ is the loss margin hyperparameter.
By training the model to pull cited papers closer in vector space and push unrelated papers apart, 2 learns embeddings that reflect the functional relationships between papers.

\subsection{Vector Database Indexing}

Searching a dataset of millions of high-dimensional vectors is computationally prohibitive using brute force ($O(N)$).
SCET relies on Approximate Nearest Neighbor (ANN) algorithms.

\subsubsection{Inverted File Index}
The Inverted File Index (IVF) works by partitioning the vector space into Voronoi cells, where every document vector is then assigned to its nearest centroid.
Now upon search, the system can extract a subset of the vectors by comparing the query vector to the centroids.
A benefit to IVF is its low memory footprint \cite{ir:manning}.

\subsubsection{Hierarchical Navigable Small World}
While partition-based methods like IVF offer memory efficiency, graph-based approaches currently provide the superior trade-off between latency and recall.
Hierarchical Navigable Small World (HNSW) structures data into a multi-layered graph hierarchy inspired by Skip Lists and the ``small world'' phenomenon \cite{small-world}.
The upper layers consist of sparse, long-range links that allow the search algorithm to traverse the vector space rapidly, effectively zooming in on the target region.
Once the coarse location is identified, the search descends to lower, denser layers for fine-grained greedy traversal to locate the nearest neighbors.
Although HNSW requires higher memory overhead to store the graph connectivity compared to quantization methods, it is robust against the curse of dimensionality and does not require the training phases of clustering approaches \cite{hnsw}.

\subsection{Semantic Drift Analysis}

Semantic drift (or semantic change) is the study of change with respect to the meaning of words, specifically the evolution of how a word is used.
For example, the word \textit{awful} originally meant to inspire wonder or fear, and hence impressive.
Today, it is used to describe something that is regarded as very bad.

\subsubsection{Alignment}
Since embedding models are initialized randomly, the vector spaces for different time periods often end up rotated or flipped relative to each other. Orthogonal Procrustes Analysis is used to fix this, by mathematically rotating the vector space of one time period to align it with the next. By locking the two maps together, it ensures that if a word's position changes, it represents a genuine shift in meaning rather than a side effect of the model's random initialization \cite{procrustes} .

\subsubsection{Dynamic Word Embeddings}
While alignment-based methods rely on independent training of temporal slices, Dynamic Probabilistic Models treat the evolution of semantic meaning as a continuous latent variable process.
First introduced by Balmer and Mandt \cite{dynamicwords}, their process involves training a single global model where the embedding of a term at time $t$ is conditioned on its embedding at time $t-1$, modeled as a Gaussian Random Walk. This process ensures that meanings shift gradually rather than abruptly.
By using data from surrounding time periods, these models work much better for rare terms.
This eliminates the random noise often seen in year-by-year training, resulting in a clearer, smoother path of how a concept has changed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}

\subsection{System Architecture and Data Pipeline}
\subsubsection{Data Ingestion Layer}
\subsubsection{Vector Storage Layer (Milvus)}

\subsection{Hybrid Embedding Strategy}
\subsubsection{Dense Embedding (Semantic)}
\subsubsection{Sparse Embedding (Lexical)}
\subsubsection{Hybrid Scoring}

\subsection{Concept Clustering Algorithm}

\subsection{Era Detection}

\subsection{Identifying Pivotal Papers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Results}

\subsection{System Performance Benchmarks}
\subsubsection{Ingestion Scalability}
\subsubsection{Query Latency}

\subsection{Case Studies}
\subsubsection{Transformer}
\subsubsection{Corona}

\subsection{Ablation Study: Hybrid vs. Dense-Only}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

\subsection{The Time Machine Effect}

\subsection{Scalability vs. Depth Trade-off}

\subsection{Limitations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
  Acknowledgements go here. Delete enclosing begin/end markers if there are no acknowledgements.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%%
\end{document}
\endinput
%%
%% End of file `main.tex'.
